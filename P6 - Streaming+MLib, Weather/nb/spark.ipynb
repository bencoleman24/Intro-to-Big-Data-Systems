{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1087d19c-e1a0-4c1c-8fe3-a79b8f7f1a94",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50dfd408-a2f6-4512-8aae-2cef5ae35c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-42d7d2f8-2015-47e2-83a8-1e430a67a5f4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1210ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-42d7d2f8-2015-47e2-83a8-1e430a67a5f4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/16ms)\n",
      "23/04/27 22:20:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f34ab9d-9c9b-4daf-8364-ff920f3de9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import col, from_json, date_add, expr, month, when\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"station\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"degrees\", FloatType(), True),\n",
    "    StructField(\"raining\", IntegerType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db28cb5e-37b2-45cb-a73b-005a897b41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")).select(\"value.*\")\n",
    "\n",
    "stats = (\n",
    "    data.groupBy(\"station\")\n",
    "    .agg(\n",
    "        expr(\"min(date) as start\"),\n",
    "        expr(\"max(date) as end\"),\n",
    "        expr(\"count(*) as measurements\"),\n",
    "        expr(\"avg(degrees) as avg\"),\n",
    "        expr(\"max(degrees) as max\")\n",
    "    )\n",
    "    .orderBy(\"station\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d802621a-df40-4cd5-97ae-1293d1bee592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:21:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0835e34d-a776-4715-8dc7-c4eeeb1faec8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/27 22:21:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2025-05-19|        9271| 58.54733821597638| 112.31312|\n",
      "|      B|2000-01-01|2025-05-19|        9271| 64.24334486571604| 114.48652|\n",
      "|      C|2000-01-01|2025-05-19|        9271| 60.15539362681052| 114.12806|\n",
      "|      D|2000-01-01|2025-05-19|        9271|53.667564692419525| 104.69655|\n",
      "|      E|2000-01-01|2025-05-19|        9271| 48.02053792793322|102.036255|\n",
      "|      F|2000-01-01|2025-05-19|        9271| 55.63299546107005| 109.81366|\n",
      "|      G|2000-01-01|2025-05-19|        9271|43.290733723741454|  99.80991|\n",
      "|      H|2000-01-01|2025-05-19|        9271|55.166817879843336|108.422554|\n",
      "|      I|2000-01-01|2025-05-19|        9271| 70.98507145104085| 123.86119|\n",
      "|      J|2000-01-01|2025-05-19|        9271| 70.61874282358791| 119.74353|\n",
      "|      K|2000-01-01|2025-05-19|        9271| 56.15487329822575|  109.8702|\n",
      "|      L|2000-01-01|2025-05-19|        9271| 64.72522526473472| 120.75461|\n",
      "|      M|2000-01-01|2025-05-19|        9271| 44.26587527758234|104.376015|\n",
      "|      N|2000-01-01|2025-05-19|        9271|  62.5154309314427| 115.94982|\n",
      "|      O|2000-01-01|2025-05-19|        9271| 62.92112154292515|  110.9521|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:21:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 18826 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2025-06-03|        9286|58.577163748435105| 112.31312|\n",
      "|      B|2000-01-01|2025-06-03|        9286| 64.26074389222109| 114.48652|\n",
      "|      C|2000-01-01|2025-06-03|        9286| 60.18989694795037| 114.12806|\n",
      "|      D|2000-01-01|2025-06-03|        9286| 53.70581512151384| 104.69655|\n",
      "|      E|2000-01-01|2025-06-03|        9286| 48.02167179496791|102.036255|\n",
      "|      F|2000-01-01|2025-06-03|        9286| 55.66624028670385| 109.81366|\n",
      "|      G|2000-01-01|2025-06-03|        9286|43.322915709199656|  99.80991|\n",
      "|      H|2000-01-01|2025-06-03|        9286|55.190007411952855|108.422554|\n",
      "|      I|2000-01-01|2025-06-03|        9286| 71.01117011829616| 123.86119|\n",
      "|      J|2000-01-01|2025-06-03|        9286| 70.66326981712307| 119.74353|\n",
      "|      K|2000-01-01|2025-06-03|        9286|  56.1673744170879|  109.8702|\n",
      "|      L|2000-01-01|2025-06-03|        9286| 64.75157194514483| 120.75461|\n",
      "|      M|2000-01-01|2025-06-03|        9286| 44.29552943717281|104.376015|\n",
      "|      N|2000-01-01|2025-06-03|        9286| 62.54279599096301| 115.94982|\n",
      "|      O|2000-01-01|2025-06-03|        9286| 62.94328385639006|  110.9521|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2025-06-06|        9289| 58.58378892030099| 112.31312|\n",
      "|      B|2000-01-01|2025-06-06|        9289| 64.26657848037833| 114.48652|\n",
      "|      C|2000-01-01|2025-06-06|        9289| 60.20389827865486| 114.12806|\n",
      "|      D|2000-01-01|2025-06-06|        9289|53.711554563484086| 104.69655|\n",
      "|      E|2000-01-01|2025-06-06|        9289|48.024515387353006|102.036255|\n",
      "|      F|2000-01-01|2025-06-06|        9289| 55.67455931694929| 109.81366|\n",
      "|      G|2000-01-01|2025-06-06|        9289| 43.33333876261422|  99.80991|\n",
      "|      H|2000-01-01|2025-06-06|        9289|55.195994031722535|108.422554|\n",
      "|      I|2000-01-01|2025-06-06|        9289|  71.0216278726465| 123.86119|\n",
      "|      J|2000-01-01|2025-06-06|        9289| 70.67000084260088| 119.74353|\n",
      "|      K|2000-01-01|2025-06-06|        9289|56.171168935513556|  109.8702|\n",
      "|      L|2000-01-01|2025-06-06|        9289| 64.75292271337797| 120.75461|\n",
      "|      M|2000-01-01|2025-06-06|        9289| 44.30416691389222|104.376015|\n",
      "|      N|2000-01-01|2025-06-06|        9289|62.549680688689456| 115.94982|\n",
      "|      O|2000-01-01|2025-06-06|        9289| 62.94858815634337|  110.9521|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:21:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3cc4cd53 is aborting.\n",
      "23/04/27 22:21:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3cc4cd53 aborted.\n"
     ]
    }
   ],
   "source": [
    "s = stats.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be9b648-fa9c-46c4-8a90-dbb9d101ccd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "today = data.select(\"station\", \"date\", \"raining\")\n",
    "yesterday = data.select(\"station\", date_add(\"date\", 1).alias(\"date\"), \"degrees\", \"raining\").withColumnRenamed(\"degrees\", \"sub1degrees\").withColumnRenamed(\"raining\", \"sub1raining\")\n",
    "two_days_ago = data.select(\"station\", date_add(\"date\", 2).alias(\"date\"), \"degrees\", \"raining\").withColumnRenamed(\"degrees\", \"sub2degrees\").withColumnRenamed(\"raining\", \"sub2raining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f47fe47-4734-4f5b-ac8f-d8a0a9e0dfe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:21:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"checkpoint\")\n",
    "output_path = os.path.join(os.getcwd(), \"output\")\n",
    "\n",
    "\n",
    "features = (\n",
    "    today.join(yesterday, [\"station\", \"date\"])\n",
    "    .join(two_days_ago, [\"station\", \"date\"])\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .select(\"station\", \"date\", \"month\", \"raining\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\")\n",
    ")\n",
    "\n",
    "stream_query = (\n",
    "    features.repartition(1)\n",
    "    .writeStream.format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", output_path)\n",
    "    .trigger(processingTime=\"1 minutes\")\n",
    "    .start()\n",
    ")\n",
    "stream_query.awaitTermination(30)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640014c-bb61-4a83-b1bd-888e8ecb1c8c",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e11346-bcc3-4658-80e8-aeff7287b64a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_6c49595bb873, depth=5, numNodes=23, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 1 <= 37.52163887023926)\n",
      "    If (feature 0 <= 2.5)\n",
      "     If (feature 3 <= 32.25822067260742)\n",
      "      If (feature 3 <= 21.409591674804688)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 21.409591674804688)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 32.25822067260742)\n",
      "      If (feature 3 <= 47.68816947937012)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 47.68816947937012)\n",
      "       Predict: 1.0\n",
      "    Else (feature 0 > 2.5)\n",
      "     If (feature 0 <= 11.5)\n",
      "      If (feature 1 <= 35.317331314086914)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 35.317331314086914)\n",
      "       Predict: 1.0\n",
      "     Else (feature 0 > 11.5)\n",
      "      If (feature 3 <= 45.004194259643555)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 45.004194259643555)\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 > 37.52163887023926)\n",
      "    If (feature 1 <= 39.87957000732422)\n",
      "     If (feature 0 <= 1.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 > 1.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 1 > 39.87957000732422)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "data = spark.read.parquet(output_path)\n",
    "assembler = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "data_features = assembler.transform(data)\n",
    "train, test = data_features.randomSplit([0.8, 0.2], seed=20)\n",
    "\n",
    "classifier = DecisionTreeClassifier(labelCol=\"raining\", featuresCol=\"features\")\n",
    "model = classifier.fit(train)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab455a6-2a64-4b46-aedc-cda13379bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage accuracy: 79.09810870429972%\n",
      "Percentage raining: 33.707011897443316%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"raining\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "total = predictions.count()\n",
    "raining_records = predictions.filter(predictions[\"raining\"] == 1).count()\n",
    "percentage_raining = (raining_records / total) * 100\n",
    "\n",
    "print(f\"Percentage accuracy: {accuracy * 100}%\")\n",
    "print(f\"Percentage raining: {percentage_raining}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ceb679f-a3e8-4da7-be5f-644728de8f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/27 22:22:22 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-44e30da0-10bc-4856-b512-aab43bc4ca35. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/27 22:22:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      J|2000-01-13|       1.0|\n",
      "|      I|2000-01-16|       0.0|\n",
      "|      J|2000-01-18|       0.0|\n",
      "|      J|2000-01-19|       0.0|\n",
      "|      I|2000-01-20|       0.0|\n",
      "|      J|2000-01-31|       0.0|\n",
      "|      I|2000-02-03|       0.0|\n",
      "|      J|2000-02-06|       1.0|\n",
      "|      J|2000-02-11|       0.0|\n",
      "|      I|2000-02-13|       0.0|\n",
      "|      J|2000-02-13|       0.0|\n",
      "|      F|2000-02-14|       0.0|\n",
      "|      J|2000-02-15|       0.0|\n",
      "|      F|2000-02-18|       0.0|\n",
      "|      J|2000-02-22|       0.0|\n",
      "|      I|2000-03-02|       1.0|\n",
      "|      I|2000-03-07|       1.0|\n",
      "|      J|2000-03-08|       1.0|\n",
      "|      F|2000-03-15|       1.0|\n",
      "|      I|2000-03-16|       0.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_query = (\n",
    "    model.transform(assembler.transform(features))\n",
    "    .select(\"station\", \"date\", \"prediction\")\n",
    "    .writeStream.format(\"console\")\n",
    "    .trigger(processingTime=\"1 minutes\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "prediction_query.awaitTermination(30)\n",
    "prediction_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
